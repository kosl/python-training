{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://docs.dask.org/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n",
    "\n",
    "## Parallelize code with `dask.delayed`\n",
    "\n",
    "In this notebook we parallelize simple for-loop style code with Dask and `dask.delayed`. Often, this is the only function that you will need to convert functions for use with Dask.\n",
    "\n",
    "We will then go on and take a look at Dask DataFrames and how they compute\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "* [Delayed documentation](https://docs.dask.org/en/latest/delayed.html)\n",
    "* [Delayed screencast](https://www.youtube.com/watch?v=SHqFmynRxVU)\n",
    "* [Delayed API](https://docs.dask.org/en/latest/delayed-api.html)\n",
    "* [Delayed examples](https://examples.dask.org/delayed.html)\n",
    "* [Delayed best practices](https://docs.dask.org/en/latest/delayed-best-practices.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we'll see in the [distributed scheduler notebook](05_distributed.ipynb), Dask has several ways of executing code in parallel. For now we will just use the default implementation to let the tasks run locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's make some toy functions, `inc` and `add`, that sleep for a while to simulate work. We'll then time running these functions normally.\n",
    "\n",
    "In the next section we'll parallelize this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def inc(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "def add(x, y):\n",
    "    sleep(1)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We time the execution of this normal code using the `%%time` magic, which is a special function of the Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.84 ms, sys: 3.97 ms, total: 7.81 ms\n",
      "Wall time: 3.02 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# This takes three seconds to run because we call each\n",
    "# function sequentially, one after the other\n",
    "\n",
    "x = inc(1)\n",
    "y = inc(2)\n",
    "z = add(x, y)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those two increment calls *could* be called in parallel, because they are totally independent of one-another.\n",
    "\n",
    "We'll transform the `inc` and `add` functions using the `dask.delayed` function. When we call the delayed version by passing the arguments, exactly as before, the original function isn't actually called yet - which is why the cell execution finishes very quickly.\n",
    "Instead, a *delayed object* is made, which keeps track of the function to call and the arguments to pass to it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delay, visualize, compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.95 ms, sys: 305 µs, total: 2.25 ms\n",
      "Wall time: 2.74 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This runs immediately, as it does not compute anything.\n",
    "# All it does is build a graph.\n",
    "\n",
    "x = delayed(inc)(1)\n",
    "y = delayed(inc)(2)\n",
    "z = delayed(add)(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ran immediately, since nothing has really happened yet.\n",
    "\n",
    "To get the result, call `compute`. Notice that this runs faster than the original code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.11 ms, sys: 2.24 ms, total: 3.35 ms\n",
      "Wall time: 2.01 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# This actually runs our computation using a local thread pool\n",
    "\n",
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `z` object is a lazy `Delayed` object.  This object holds everything we need to compute the final result, including references to all of the functions that are required and their inputs and relationship to one-another.  We can evaluate the result with `.compute()` as above or we can visualize the task graph for this value with `.visualize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Delayed('add-528c9830-00a2-48fc-bdb7-edf0e2998cd4')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting graphviz\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 1.8 MB/s eta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: graphviz\n",
      "Successfully installed graphviz-0.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the task graph for `z`\n",
    "%pip install graphviz\n",
    "z.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this includes the names of the functions from before, and the logical flow of the outputs of the `inc` functions to the inputs of `add`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some questions to consider:\n",
    "\n",
    "-  Why did we go from 3s to 2s?  Why weren't we able to parallelize down to 1s?\n",
    "-  What would have happened if the inc and add functions didn't include the `sleep(1)`?  Would Dask still be able to speed up this code?\n",
    "-  What if we have multiple outputs or also want to get access to x or y?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc(x):\n",
    "    #sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "def add(x, y):\n",
    "    #sleep(1)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34 µs, sys: 5 µs, total: 39 µs\n",
      "Wall time: 44.3 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 3, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "x = inc(1)\n",
    "y = inc(2)\n",
    "z = add(x, y)\n",
    "x,y,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.07 ms, sys: 31 µs, total: 3.1 ms\n",
      "Wall time: 2.56 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 3, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "x = delayed(inc)(1)\n",
    "y = delayed(inc)(2)\n",
    "z = delayed(add)(x, y)\n",
    "dask.compute(x,y,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Parallelize a for loop\n",
    "\n",
    "`for` loops are one of the most common things that we want to parallelize.  Use `dask.delayed` on `inc` and `sum` to parallelize the computation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5, 6, 7, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def inc(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "def add(x, y):\n",
    "    sleep(1)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Sequential code\n",
    "\n",
    "results = []\n",
    "for x in data:\n",
    "    y = inc(x)\n",
    "    results.append(y)\n",
    "    \n",
    "total = sum(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Your parallel code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "results = []\n",
    "\n",
    "for x in data:\n",
    "    y = delayed(inc)(x)\n",
    "    results.append(y)\n",
    "    \n",
    "total = delayed(sum)(results)\n",
    "print(\"Before computing:\", total)  # Let's see what total is\n",
    "result = total.compute()\n",
    "print(\"After computing :\", result)  # After it's computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the graph visualizations compare with the given solution, compared to a version with the `sum` function used directly rather than wrapped with `delayed`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.visualize() # sum function wrapped with delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = []\n",
    "\n",
    "for x in data:\n",
    "    y = delayed(inc)(x)\n",
    "    results.append(y)\n",
    "    \n",
    "total = sum(results)\n",
    "\n",
    "total.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dask DataFrames\n",
    "In this section we use `dask.dataframe` to automatically build similar computations as we just performed, for the common case of tabular computations. Dask dataframes look and feel like Pandas dataframes but they run on the same infrastructure that powers dask.delayed.\n",
    "\n",
    "In this chapter we use the same airline data as before, but now rather than write for-loops we let dask.dataframe construct our computations for us. The `dask.dataframe.read_csv` function can take a globstring like \"data/nycflights/*.csv\" and build parallel computations on all of our data at once.\n",
    "\n",
    "Pandas is great for tabular datasets that fit in memory. Dask becomes useful when the dataset you want to analyse is larger than your machine's RAM. Our dataset does not bring the avaliable memory to its limits, but we can still demonstrate how you go about making use of `dask.dataframe`.\n",
    "\n",
    "The `dask.dataframe` module implements a blocked parallel `DataFrame` object that mimics a large subset of the Pandas `DataFrame` API. One Dask `DataFrame` is comprised of many in-memory pandas `DataFrames` separated along the index. One operation on a Dask `DataFrame` triggers many pandas operations on the constituent pandas `DataFrame`s in a way that is mindful of potential parallelism and memory constraints.\n",
    "\n",
    "**Main Take-aways**\n",
    "\n",
    "1.  Dask DataFrame should be familiar to Pandas users\n",
    "2.  The partitioning of dataframes is important for efficient execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask DataFrame Data Model\n",
    "\n",
    "For the most part, a Dask DataFrame feels like a pandas DataFrame.\n",
    "So far, the biggest difference we've seen is that Dask operations are lazy; they build up a task graph instead of executing immediately.\n",
    "This lets Dask do operations in parallel and out of core.\n",
    "\n",
    "A Dask DataFrame is composed of many pandas DataFrames. For `dask.dataframe` the chunking happens along the index.\n",
    "\n",
    "<img src=\"http://docs.dask.org/en/latest/_images/dask-dataframe.svg\" width=\"30%\">\n",
    "\n",
    "We call each chunk a *partition*, and the upper / lower bounds are *divisions*.\n",
    "Dask *can* store information about the divisions. For now, partitions come up when you write custom functions to apply to Dask DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_csv(\"./data/nycflights/1999.csv\") # With Dask, you can also enter a url instead of a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf.head(3) # This works just as in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.tail(3) # And so does this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watch out for wrongly interpreted data types in Dask! Dask is lazy in every way and you possibly need to manually enter some data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mulitple Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask can intelligently read multiple files into one dataframe with a glob (asterisk):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filepath = glob(\"./data/nycflights/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head(3) # this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ddf.tail(3) # This fails. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.dtypes # Let's check the datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike `pandas.read_csv` which reads in the entire file before inferring datatypes, `dask.dataframe.read_csv` only reads in a sample from the beginning of the file (or first file if using a glob). These inferred datatypes are then enforced when reading all partitions.\n",
    "\n",
    "In this case, the datatypes inferred in the sample are incorrect. The first `n` rows have no value for `CRSElapsedTime` (which pandas infers as a `float`), and later on turn out to be strings (`object` dtype). Note that Dask gives an informative error message about the mismatch. When this happens you have a few options:\n",
    "\n",
    "- Specify dtypes directly using the `dtype` keyword. This is the recommended solution, as it's the least error prone (better to be explicit than implicit) and also the most performant.\n",
    "- Increase the size of the `sample` keyword (in bytes)\n",
    "- Use `assume_missing` to make `dask` assume that columns inferred to be `int` (which don't allow missing values) are actually floats (which do allow missing values). In our particular case this doesn't apply.\n",
    "\n",
    "In our case we'll use the first option and directly specify the `dtypes` of (just) the offending columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),\n",
    "                 parse_dates={'Date': [0, 1, 2]}, # Here we parse the year, month and day into date\n",
    "                 dtype={'TailNum': str,\n",
    "                        'CRSElapsedTime': float,\n",
    "                        'Cancelled': bool})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the respresentation of the dataframe object contains no data - Dask has just done enough to read the start of the first file, and infer the column names and dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.tail(3) # Now it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also read the holidays data which will use in the exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = dd.read_parquet(os.path.join('data', \"holidays\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "holidays.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Computations with `dask.dataframe`\n",
    "\n",
    "We compute the maximum of the `DepDelay` column. With just pandas, we would loop over each file to find the individual maximums, then find the final maximum over all the individual maximums\n",
    "\n",
    "```python\n",
    "maxes = []\n",
    "for fn in filenames:\n",
    "    pdf = pd.read_csv(fn)\n",
    "    maxes.append(df[\"DepDelay\"].max())\n",
    "    \n",
    "final_max = max(maxes)\n",
    "```\n",
    "\n",
    "We could wrap that `pd.read_csv` with `dask.delayed` so that it runs in parallel. Regardless, we're still having to think about loops, intermediate results (one per file) and the final reduction (`max` of the intermediate maxes). This is just noise around the real task, which pandas solves with\n",
    "\n",
    "```python\n",
    "pdf = pd.read_csv(filename, dtype=dtype)\n",
    "pdf[\"DepDelay\"].max()\n",
    "```\n",
    "\n",
    "`dask.dataframe` lets us write pandas-like code, that operates on larger than memory datasets in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the performance of data loading and computation to pandas.\n",
    "First we are going to time this in Dask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),\n",
    "                 parse_dates={'Date': [0, 1, 2]}, # Here we parse the year, month and day into date\n",
    "                 dtype={'TailNum': str,\n",
    "                        'CRSElapsedTime': float,\n",
    "                        'Cancelled': bool})\n",
    "ddf[\"DepDelay\"].max().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this to pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filepath = glob(\"./data/nycflights/*.csv\")\n",
    "pdf = pd.concat(pd.read_csv(f) for f in filepath)\n",
    "pdf[\"DepDelay\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.concat(pd.read_csv(f) for f in filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pdf[\"DepDelay\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This writes the delayed computation for us and then runs it.  \n",
    "\n",
    "Some things to note:\n",
    "\n",
    "1.  As with `dask.delayed`, we need to call `.compute()` when we're done.  Up until this point everything is lazy.\n",
    "2.  Dask will delete intermediate results (like the full pandas dataframe for each file) as soon as possible.\n",
    "    -  This lets us handle datasets that are larger than memory\n",
    "    -  This means that repeated computations will have to load all of the data in each time (run the code above again, is it faster or slower than you would expect?)\n",
    "    \n",
    "As with `Delayed` objects, you can view the underlying task graph using the `.visualize` method (notice the parallelism):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.DepDelay.max().visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "In this section we do a few `dask.dataframe` computations. If you are comfortable with pandas then these should be familiar. You will have to think about when to call `compute`.\n",
    "\n",
    "#### 1.) How many rows are in our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.) In total, how many non-canceled flights were taken?\n",
    "\n",
    "With pandas, you would use [boolean indexing](https://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(ddf[ddf.Cancelled==False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.) What was the average departure delay from each airport?\n",
    "*Hint*: use [`ddf.groupby`](https://pandas.pydata.org/pandas-docs/stable/groupby.html).\n",
    "\n",
    "Note, this is the same computation you did in the previous notebook (is this approach faster or slower?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time ddf[\"DepDelay\"].groupby(ddf[\"Origin\"]).mean().compute()\n",
    "# Alternative solution:\n",
    "# ddf.groupby(\"Origin\").DepDelay.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.) In total, how many non-cancelled flights were taken from each airport?\n",
    "\n",
    "*Hint*: use [`ddf.groupby`](https://pandas.pydata.org/pandas-docs/stable/groupby.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf[~ddf.Cancelled].groupby(\"Origin\").Origin.count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.) What day of the week has the worst average departure delay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf[\"DepDelay\"].groupby(ddf[\"DayOfWeek\"]).mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.) What holiday has the worst average departure delay?\n",
    "\n",
    "*Hint*: use [`ddf.merge`](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html) to bring holiday information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time ddf.merge(holidays, on=[\"Date\"], how=\"left\").groupby(\"holiday\").DepDelay.mean().compute()\n",
    "\n",
    "# Alternative Solution:\n",
    "#%%time\n",
    "#ddf_merged = ddf.merge(holidays, on=[\"Date\"], how=\"left\")\n",
    "#ddf_merged[\"DepDelay\"].groupby(ddf_merged[\"holiday\"]).mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Sharing Intermediate Results\n",
    "\n",
    "When computing all of the above, we sometimes did the same operation more than once. For most operations, `dask.dataframe` hashes the arguments, allowing duplicate computations to be shared, and only computed once.\n",
    "\n",
    "For example, lets compute the mean and standard deviation for departure delay of all non-canceled flights. Since dask operations are lazy, those values aren't the final results yet. They're just the recipe required to get the result.\n",
    "\n",
    "If we compute them with two calls to compute, there is no sharing of intermediate computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cancelled = ddf[~ddf.Cancelled]\n",
    "mean_delay = non_cancelled.DepDelay.mean()\n",
    "std_delay = non_cancelled.DepDelay.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mean_delay_res = mean_delay.compute()\n",
    "std_delay_res = std_delay.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's try by passing both to a single `compute` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mean_delay_res, std_delay_res = compute(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `dask.compute` takes roughly 1/2 the time. This is because the task graphs for both results are merged when calling `dask.compute`, allowing shared operations to only be done once instead of twice. In particular, using `dask.compute` only does the following once:\n",
    "\n",
    "- the calls to `read_csv`\n",
    "- the filter (`df[~df.Cancelled]`)\n",
    "- some of the necessary reductions (`sum`, `count`)\n",
    "\n",
    "To see what the merged task graphs between multiple results look like (and what's shared), you can use the `dask.visualize` function (we might want to use `filename='graph.pdf'` to save the graph to disk so that we can zoom in more easily):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(mean_delay, std_delay, filename=\"graph.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### How does this compare to Pandas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is more mature and fully featured than `dask.dataframe`.  If your data fits in memory then you should use Pandas.  The `dask.dataframe` module gives you a limited `pandas` experience when you operate on datasets that don't fit comfortably in memory.\n",
    "\n",
    "This dataset is small enough that you would normally use Pandas.\n",
    "\n",
    "We've chosen this size so that exercises finish quickly. Dask.dataframe only really becomes meaningful for problems significantly larger than this, when Pandas breaks with the dreaded \n",
    "\n",
    "    MemoryError:  ...\n",
    "    \n",
    "Furthermore, the distributed scheduler allows the same dataframe expressions to be executed across a cluster. To enable massive \"big data\" processing, one could execute data ingestion functions such as `read_csv`, where the data is held on storage accessible to every worker node, and because most operations begin by selecting only some columns, transforming and filtering the data, only relatively small amounts of data need to be communicated between the machines.\n",
    "\n",
    "Dask.dataframe operations use `pandas` operations internally. Generally they run at about the same speed except in the following two cases:\n",
    "\n",
    "1.  Dask introduces a bit of overhead, around 1ms per task.  This is usually negligible.\n",
    "2.  When Pandas releases the GIL (global interpreter lock) `dask.dataframe` can call several pandas operations in parallel within a process, increasing speed somewhat proportional to the number of cores. For operations which don't release the GIL, multiple processes would be needed to get the same speedup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Conversion to a timestamp\n",
    "\n",
    "This dataset stores timestamps as `HHMM`, which are read in as integers in `read_csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_dep_time = pdf[\"CRSDepTime\"].head(10)\n",
    "crs_dep_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert these to timestamps of scheduled departure time, we need to convert these integers into `pd.Timedelta` objects, and then combine them with the `Date` column.\n",
    "\n",
    "In the pandas notebook, we rounded and converted the data to integers, then to strings and then to the date_time datatype. However, we could also do this using the `pd.to_timedelta` function, and a bit of arithmetic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Rename column \"DayofMonth\"\n",
    "pdf.rename(columns = {\"DayofMonth\":\"Day\"}, inplace=True)\n",
    "\n",
    "# Parse to Date:\n",
    "pdf[\"Date\"] = pd.to_datetime(pdf[[\"Year\",\"Month\",\"Day\"]])\n",
    "\n",
    "# Get the first 10 dates to complement our `crs_dep_time`\n",
    "date = pdf.Date.head(10)\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hours as an integer, convert to a timedelta\n",
    "hours = crs_dep_time // 100\n",
    "hours_timedelta = pd.to_timedelta(hours, unit='h')\n",
    "\n",
    "# Get minutes as an integer, convert to a timedelta\n",
    "minutes = crs_dep_time % 100\n",
    "minutes_timedelta = pd.to_timedelta(minutes, unit='m')\n",
    "\n",
    "# Apply the timedeltas to offset the dates by the departure time\n",
    "departure_timestamp = date + hours_timedelta + minutes_timedelta\n",
    "departure_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom code and Dask Dataframe\n",
    "\n",
    "We could swap out `pd.to_timedelta` for `dd.to_timedelta` and do the same operations on the entire dask DataFrame. But let's say that Dask hadn't implemented a `dd.to_timedelta` that works on Dask DataFrames. What would you do then?\n",
    "\n",
    "`dask.dataframe` provides a few methods to make applying custom functions to Dask DataFrames easier:\n",
    "\n",
    "- [`map_partitions`](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.map_partitions)\n",
    "- [`map_overlap`](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.map_overlap)\n",
    "- [`reduction`](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.reduction)\n",
    "\n",
    "Here we'll just be discussing `map_partitions`, which we can use to implement `to_timedelta` on our own:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is to apply a function that operates on a DataFrame to each partition.\n",
    "In this case, we'll apply `pd.to_timedelta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours = ddf.CRSDepTime // 100\n",
    "# hours_timedelta = pd.to_timedelta(hours, unit='h')\n",
    "hours_timedelta = hours.map_partitions(pd.to_timedelta, unit='h')\n",
    "\n",
    "minutes = ddf.CRSDepTime % 100\n",
    "# minutes_timedelta = pd.to_timedelta(minutes, unit='m')\n",
    "minutes_timedelta = minutes.map_partitions(pd.to_timedelta, unit='m')\n",
    "\n",
    "departure_timestamp = ddf.Date + hours_timedelta + minutes_timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departure_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departure_timestamp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rewrite above to use a single call to `map_partitions`\n",
    "\n",
    "This will be slightly more efficient than two separate calls, as it reduces the number of tasks in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_departure_timestamp(ddf):\n",
    "    hours = ddf.CRSDepTime // 100\n",
    "    hours_timedelta = pd.to_timedelta(hours, unit='h')\n",
    "\n",
    "    minutes = ddf.CRSDepTime % 100\n",
    "    minutes_timedelta = pd.to_timedelta(minutes, unit='m')\n",
    "\n",
    "    return ddf.Date + hours_timedelta + minutes_timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departure_timestamp = ddf.map_partitions(compute_departure_timestamp)\n",
    "departure_timestamp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask.dataframe only covers a small but well-used portion of the Pandas API.\n",
    "This limitation is for two reasons:\n",
    "\n",
    "1.  The Pandas API is *huge*\n",
    "2.  Some operations are genuinely hard to do in parallel (e.g. sort)\n",
    "\n",
    "Additionally, some important operations like ``set_index`` work, but are slower\n",
    "than in Pandas because they include substantial shuffling of data, and may write out to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn More\n",
    "\n",
    "\n",
    "* [DataFrame documentation](https://docs.dask.org/en/latest/dataframe.html)\n",
    "* [DataFrame screencast](https://youtu.be/AT2XtFehFSQ)\n",
    "* [DataFrame API](https://docs.dask.org/en/latest/dataframe-api.html)\n",
    "* [DataFrame examples](https://examples.dask.org/dataframe.html)\n",
    "* [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Array\n",
    "\n",
    "Parallel, larger-than-memory, n-dimensional array using blocked algorithms.\n",
    "\n",
    "* Parallel: Uses all of the cores on your computer\n",
    "* Larger-than-memory: Lets you work on datasets that are larger than your available memory by breaking up your array into many small pieces, operating on those pieces in an order that minimizes the memory footprint of your computation, and effectively streaming data from disk.\n",
    "* Blocked Algorithms: Perform large computations by performing many smaller computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import numpy as np\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + x.T\n",
    "z = y[::2, 5000:].mean(axis=1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare Numpy to Dask. First we time the numpy version of a computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "xn = np.random.normal(10, 0.1, size=(30_000, 30_000)) \n",
    "yn = xn.mean(axis=0)\n",
    "yn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del xn, yn # Let's release some memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the Dask version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xd = da.random.normal(10, 0.1, size=(30_000, 30_000), chunks=(3000, 3000))\n",
    "xd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xd.nbytes / 1e9  # Gigabytes of the input processed lazily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yd = xd.mean(axis=0) \n",
    "yd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xd = da.random.normal(10, 0.1, size=(30_000, 30_000), chunks=(3000, 3000))\n",
    "yd = xd.mean(axis=0) \n",
    "yd.compute() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions to think about:**\n",
    "\n",
    "* What happens if the Dask chunks=(10000,10000)?\n",
    "* What happens if the Dask chunks=(30,30)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "For Dask arrays, compute the mean along axis=1 of the sum of the x array and its transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_sum = xd + xd.T \n",
    "res = x_sum.mean(axis=1)\n",
    "res.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del xd, yd # Another memory release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Persist Data in Memory\n",
    "\n",
    "If you have the available RAM for your dataset then you can persist data in memory.\n",
    "This allows future computations to be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = y.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y[0, 0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen so far, Dask allows you to simply construct graphs of tasks with dependencies, as well as have graphs created automatically for you using functional, Numpy or Pandas syntax on data collections. None of this would be very useful, if there weren't also a way to execute these graphs, in a parallel and memory-aware way. So far we have been calling `thing.compute()` or `dask.compute(thing)` without worrying what this entails. Now we will discuss the options available for that execution, and in particular, the distributed scheduler, which comes with additional functionality.\n",
    "\n",
    "Dask comes with four available schedulers:\n",
    "- \"threaded\" (aka \"threading\"): a scheduler backed by a thread pool\n",
    "- \"processes\": a scheduler backed by a process pool\n",
    "- \"single-threaded\" (aka \"sync\"): a synchronous scheduler, good for debugging\n",
    "- distributed: a distributed scheduler for executing graphs on multiple machines, see below.\n",
    "\n",
    "To select one of these for computation, you can specify at the time of asking for a result, e.g.,\n",
    "```python\n",
    "myvalue.compute(scheduler=\"single-threaded\")  # for debugging\n",
    "```\n",
    "\n",
    "You can also set a default scheduler either temporarily\n",
    "```python\n",
    "with dask.config.set(scheduler='processes'):\n",
    "    # set temporarily for this block only\n",
    "    # all compute calls within this block will use the specified scheduler\n",
    "    myvalue.compute()\n",
    "    anothervalue.compute()\n",
    "```\n",
    "\n",
    "Or globally\n",
    "```python\n",
    "# set until further notice\n",
    "dask.config.set(scheduler='processes')\n",
    "```\n",
    "\n",
    "Let's try out a few schedulers on the familiar case of the flights data but first setup up some parameters for the temp-directory & dashboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import os\n",
    "\n",
    "# We should all be using different Ports. Use 480## ##....Trainee ID\n",
    "DASHBOARD_PORT = 48005\n",
    "#####\n",
    "\n",
    "user = os.environ.get(\"USER\")\n",
    "\n",
    "dask.config.set({'temporary_directory': f'/tmp/dask-{user}'})\n",
    "\n",
    "dask.config.set({\"distributed.dashboard.link\": f\"/user/{user}/proxy/{DASHBOARD_PORT}/status\"})\n",
    "dask.config.get(\"distributed.dashboard.link\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(n_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum average non-cancelled delay grouped by Airport\n",
    "largest_delay = ddf[~ddf.Cancelled].groupby(\"Origin\").DepDelay.mean().max()\n",
    "largest_delay.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each of the following gives the same results (you can check!)\n",
    "# any surprises?\n",
    "import time\n",
    "for sch in ['threading', 'processes', 'sync', 'distributed']:\n",
    "    t0 = time.time()\n",
    "    r = largest_delay.compute(scheduler=sch)\n",
    "    t1 = time.time()\n",
    "    print(f\"{sch:>10}, {t1 - t0:0.4f} s; result: {r:0.2f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Points to Consider:\n",
    "\n",
    "The `threaded` scheduler is a fine choice for working with large datasets out-of-core on a single machine, as long as the functions being used release the [GIL](https://wiki.python.org/moin/GlobalInterpreterLock) most of the time. NumPy and pandas release the GIL in most places, so the `threaded` scheduler is the default for `dask.array` and `dask.dataframe`. The distributed scheduler, perhaps with `processes=False`, will also work well for these workloads on a single machine.\n",
    "\n",
    "For workloads that do hold the GIL, as is common with `dask.bag`(not part of this course) and custom code wrapped with `dask.delayed`, we recommend using the distributed scheduler, even on a single machine. Generally speaking, it's more intelligent and provides better diagnostics than the `processes` scheduler.\n",
    "\n",
    "https://docs.dask.org/en/latest/scheduling.html provides some additional details on choosing a scheduler.\n",
    "\n",
    "For scaling out work across a cluster, the distributed scheduler is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dask.distributed` system is composed of a single centralized scheduler and one or more worker processes. [Deploying](https://docs.dask.org/en/latest/setup.html) a remote Dask cluster involves some additional effort. But doing things locally it just involves creating a `Client` object, which lets you interact with the \"cluster\" (local threads or processes on your machine). For more information see [here](https://docs.dask.org/en/latest/setup/single-distributed.html). \n",
    "\n",
    "Note that `Client()` takes a lot of optional [arguments](https://distributed.dask.org/en/latest/local-cluster.html#api), to configure the number of processes/threads, memory limits and other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.distributed  \n",
    "import os\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to specify the hardware we want for our cluster. Note that the cores and memory are just what you want from one node. Later we will scale that to our requirements. As we would like to get through the SLURM queue as quickly as possible, it makes sense to use fewer cores and memory and and set a shorter walltime for each worker, but then scale the number of workers accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = SLURMCluster(queue=\"skylake_0096\", # this is the partition we want to use\n",
    "                       project=\"p70824\", # this is the project you are allocated to\n",
    "                       cores=4,          # number of cores one worker should use\n",
    "                       memory=\"4GB\",     # memory one worker should have avaliable\n",
    "                       walltime=\"00:05:00\",\n",
    "                       interface=\"ib0\",  # ib0 is infiniband, the fast network connection\n",
    "                       scheduler_options={\"interface\": \"ib0\",\n",
    "                                          \"dashboard_address\": f\":{DASHBOARD_PORT}\"\n",
    "                                         },\n",
    "                       job_extra_directives=['--qos=skylake_0096', '--reservation=training'])\n",
    "print(cluster.job_script()) # this is turned into a job script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to setup a client in order to see the dashboard. This also creates the scheduler which directs the workload to the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we finally scale the dask cluster to the size we require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=8)   # starts slurm jobs with dask workers in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!squeue -u $USER # It might take some time for the jobs to become visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing with the distributed client\n",
    "Let's see what our dask cluster can achieve with this computationally challenging calculation. Note that you might have to wait until sufficient workers are running in the queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = da.random.random((10_000,10_000,10), chunks=(1000,1000,5))\n",
    "y = da.random.random((10_000,10_000,10), chunks=(1000,1000,5))\n",
    "z = (da.arcsin(x) + da.arccos(y)).sum(axis=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "z.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.dask.visualize() # High level graph. Low level graph with z.visualize(), but this would be overwhelming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the maximum delay computation from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),\n",
    "                 parse_dates={'Date': [0, 1, 2]}, # Here we parse the year, month and day into date\n",
    "                 dtype={'TailNum': str,\n",
    "                        'CRSElapsedTime': float,\n",
    "                        'Cancelled': bool})\n",
    "ddf[\"DepDelay\"].max().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider some trivial calculation, such as we've used before, where we have added sleep statements in order to simulate real work being done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "import time\n",
    "\n",
    "def inc(x):\n",
    "    time.sleep(5)\n",
    "    return x + 1\n",
    "\n",
    "def dec(x):\n",
    "    time.sleep(3)\n",
    "    return x - 1\n",
    "\n",
    "def add(x, y):\n",
    "    time.sleep(7)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, creating a `Client` makes it the default scheduler. Any calls to `.compute` will use the cluster your `client` is attached to, unless you specify otherwise, as above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = delayed(inc)(1)\n",
    "y = delayed(dec)(2)\n",
    "total = delayed(add)(x, y)\n",
    "total.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to the flights computation from before, and see what happens on the dashboard (you may wish to have both the notebook and dashboard side-by-side). How did does this perform compared to before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time largest_delay.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case, this should be as fast or faster than the best case, threading, above. Why do you suppose this is? You should start your reading [here](https://distributed.dask.org/en/latest/index.html#architecture), and in particular note that the distributed scheduler was a complete rewrite with more intelligence around sharing of intermediate results and which tasks run on which worker. This will result in better performance in *some* cases, but still larger latency and overhead compared to the threaded scheduler, so there will be rare cases where it performs worse. Fortunately, the dashboard now gives us a lot more [diagnostic information](https://distributed.dask.org/en/latest/diagnosing-performance.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all you want to do is execute computations created using delayed, or run calculations based on the higher-level data collections, then that is about all you need to know to scale your work up to cluster scale. However, there is more detail to know about the distributed scheduler that will help with efficient usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Run the following computations while looking at the diagnostics page. In each case what is taking the most time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = SLURMCluster(queue=\"skylake_0096\", # this is the partition we want to use\n",
    "                       project=\"p70824\", # this is the project you are allocated to\n",
    "                       cores=4,          # number of cores one worker should use\n",
    "                       memory=\"4GB\",     # memory one worker should have avaliable\n",
    "                       walltime=\"00:05:00\",\n",
    "                       interface=\"ib0\",  # ib0 is infiniband, the fast network connection\n",
    "                       scheduler_options={\"interface\": \"ib0\",\n",
    "                                          \"dashboard_address\": f\":{DASHBOARD_PORT}\"\n",
    "                                         },\n",
    "                       job_extra_directives=['--qos=skylake_0096', '--reservation=training'])\n",
    "print(cluster.job_script()) # this is turned into a job script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of flights\n",
    "a = len(ddf)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of non-cancelled flights\n",
    "b = len(ddf[~ddf.Cancelled])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of non-cancelled flights per-airport\n",
    "c = ddf[~ddf.Cancelled].groupby(\"Origin\").Origin.count().compute()\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average departure delay from each airport?\n",
    "d = ddf[~ddf.Cancelled].groupby(\"Origin\").DepDelay.mean().compute()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average departure delay per day-of-week\n",
    "e = ddf.groupby(ddf.Date.dt.dayofweek).DepDelay.mean().compute()\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
